# -*- coding: utf-8 -*-
"""ML_Project03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_OZy3VHbBBdwuHby5IcJKkFZAfx-abkj

# Installation and Imports
"""

# Check the GPU
!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
import os.path

from keras import Model, optimizers
from keras.applications import xception
from keras.layers import GlobalAveragePooling2D, Dense
from keras.preprocessing.image import ImageDataGenerator

from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

import matplotlib.pyplot as plt
# %matplotlib inline

"""# Dataset

## Configuration Parameters
"""

img_width = 197
img_height = 197

batch_size = 120

base_path = '/content/drive/My Drive/Colab_Notebooks/data/monkey/'

train_path = base_path + 'training-data/training'
validation_path = base_path + 'validation-data/validation'

nb_classes = 10

"""## Preparing the dataset"""

# Counts the number of training and testing samples in the directories
training_samples = sum([len(files) for r, d, files in os.walk(train_path)])
validation_samples = sum([len(files) for r, d, files in os.walk(validation_path)])

print("Number of samples in Training dataset:", training_samples)
print("Number of samples in Validation dataset:", validation_samples)

"""## Data augmentation"""

train_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.2,
    rescale=1.0 / 255,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    vertical_flip=False,
)

validation_datagen = ImageDataGenerator(rescale=1.0 / 255)

train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
)

validation_generator = validation_datagen.flow_from_directory(
    validation_path,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
)

"""# Model

## Main functions
"""

# Adds new top to base model
def add_top(base):
    x = base.output

    # Global averaging pool layer
    x = GlobalAveragePooling2D()(x)

    # Regular densely connected layer
    x = Dense(512, activation='relu')(x)

    # Output layer
    predictions = Dense(nb_classes, activation='softmax')(x)

    return Model(input=base.input, output=predictions)

# Sets up model for transfer learning
def setup_model(model, base):
    # Freeze the un-trainable layers of the model base
    for layer in base.layers:
        layer.trainable = False

    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        metrics=['accuracy']
    )

"""## Importing the pre-trained model"""

# Import the Xception model to use as the base for our model
xception_base = xception.Xception(
    include_top=False,
    weights='imagenet',
    input_shape=(img_width, img_height, 3)
)

xception_base.summary()

"""## Transfer learning"""

model = add_top(xception_base)
setup_model(model, xception_base)

"""# Training

## Parameters
"""

epochs = 10

"""## Train the new layers"""

# Train our new top layer
history1 = model.fit_generator(
    train_generator,
    steps_per_epoch = training_samples // batch_size,
    epochs = epochs,
    validation_data= validation_generator,
    validation_steps = validation_samples // batch_size,
    verbose = 1,
)

"""# Fine Tune

## Main functions
"""

# Setup model for fine tuning
def setup_model(model, trainable):
    # Freeze the un-trainable layers of the model base
    for layer in model.layers[:(len(model.layers) - trainable)]:
        layer.trainable = False

    for layer in model.layers[(len(model.layers) - trainable):]:
        layer.trainable = True

    model.compile(
        loss='categorical_crossentropy',
        # Slower training rate for fine-tuning
        optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
        metrics=['accuracy']
    )

"""## Parameters"""

epochs_ft = 10

"""## Training"""

# Setup model to retrain our top layer plus block 13 and 14 of Xception
setup_model(model, 19)

# Fine-tune the model
history2 = model.fit_generator(
    train_generator,
    steps_per_epoch = training_samples // batch_size,
    epochs = epochs_ft,
    validation_data = validation_generator,
    validation_steps = validation_samples // batch_size,
    verbose=1,
)

"""# Diagrams"""

SVG(model_to_dot(model, show_shapes = False, show_layer_names=True, rankdir='HB', dpi = 72).create(prog='dot', format='svg'))

model.summary()

fig, ax = plt.subplots(1, 2, figsize = (14, 5))

history = history2

ax[0].plot(history.history["loss"])
ax[0].plot(history.history["val_loss"])
ax[0].set_title("model loss")
ax[0].set_ylabel("loss")
ax[0].set_xlabel("epoch")
ax[0].legend(["train", "test"], loc="upper left")

ax[1].plot(history.history["accuracy"])
ax[1].plot(history.history["val_accuracy"])
ax[1].set_title("model accuracy")
ax[1].set_ylabel("accuracy")
ax[1].set_xlabel("epoch")
ax[1].legend(["train", "test"], loc="upper left")

plt.show()